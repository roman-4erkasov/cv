{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebdd875c-70b8-4e3d-a948-7bc688a75dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfdadf9e-eafb-4235-80fe-ac151770dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from common import get_logger\n",
    "from dataset import DetectionDataset\n",
    "from models import get_model\n",
    "from transforms import get_train_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a51ce763-6c3b-4082-b817-f293cdc9ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        for t in self.transforms:\n",
    "            image, mask = t(image, mask)\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image, mask, force_apply=False):\n",
    "        image_, mask_ = image.copy(), mask.copy()\n",
    "        if image_.shape[0] != self.size[1] or image_.shape[1] != self.size[0]:\n",
    "            image_ = cv2.resize(image_, self.size)\n",
    "            mask_ = cv2.resize(mask_, self.size)\n",
    "        return dict(image=image_, mask=mask_)\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean=(0.5, 0.5, 0.5), std=(0.25, 0.25, 0.25)):\n",
    "        self.mean = np.asarray(mean).reshape((1, 1, 3)).astype(np.float32)\n",
    "        self.std = np.asarray(std).reshape((1, 1, 3)).astype(np.float32)\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        image = (image - self.mean) / self.std\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "# TODO TIP: Is default image size (256) enough for segmentation of car license plates?\n",
    "def get_train_transforms(image_size):\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.25, 0.25, 0.25), max_pixel_value=1.),\n",
    "        Resize(size=(image_size, image_size)),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb8e9e9d-cc17-4719-bd45-7a1386376c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/datasets/segment_car_plate/data/\"\n",
    "epochs = 8\n",
    "batch_size = 32\n",
    "image_size = 256\n",
    "output_dir = \"out\"\n",
    "lr = 3e-4\n",
    "load = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6437b2b5-438f-4032-83fe-71fb23981183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_dataloader, logger, device=None):\n",
    "    model.train()\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    tqdm_iter = tqdm.tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "    for i, batch in tqdm_iter:\n",
    "        imgs, true_masks = batch[\"image\"], batch[\"mask\"].float()\n",
    "        masks_pred = model(imgs.to(device)).float()\n",
    "        masks_probs = torch.sigmoid(masks_pred).to(device)\n",
    "\n",
    "        loss = criterion(masks_probs.view(-1), true_masks.view(-1).to(device)).cpu()\n",
    "        epoch_losses.append(loss.item())\n",
    "        tqdm_iter.set_description(f\"mean loss: {np.mean(epoch_losses):.4f}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    logger.info(f\"Epoch finished! Loss: {np.mean(epoch_losses):.5f}\")\n",
    "\n",
    "    return np.mean(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d231bfba-d7d1-44ef-8dbe-f178d30b70a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 20:27:14 Start training with params:\n",
      "2022-07-25 20:27:15 Model type: Unet\n",
      "2022-07-25 20:27:15 Length of train = 20505\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logger = get_logger(os.path.join(output_dir, \"train.log\"))\n",
    "logger.info(\"Start training with params:\")\n",
    "\n",
    "model = get_model()\n",
    "if load is not None:\n",
    "    with open(args.load, \"rb\") as fp:\n",
    "        state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "logger.info(f\"Model type: {model.__class__.__name__}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "train_transforms = get_train_transforms(image_size)\n",
    "train_dataset = DetectionDataset(data_path, os.path.join(data_path, \"train_segmentation.json\"),\n",
    "                                 transforms=train_transforms, split=\"train\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4,\n",
    "                              pin_memory=True, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "logger.info(f\"Length of train = {len(train_dataset)}\")\n",
    "best_model_info = {\"epoch\": -1, \"train_loss\": np.inf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217d63f-dc21-48a8-bdbb-1acadd269bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-25 20:27:18 Starting epoch 1/8.\n",
      "mean loss: 0.0630: 100%|████████████████████████████████████████████████| 640/640 [43:41<00:00,  4.10s/it]\n",
      "2022-07-25 21:11:00 Epoch finished! Loss: 0.06301\n",
      "2022-07-25 21:11:01 Train loss: 0.063 (best)\n",
      "2022-07-25 21:11:01 Starting epoch 2/8.\n",
      "mean loss: 0.0072: 100%|████████████████████████████████████████████████| 640/640 [40:52<00:00,  3.83s/it]\n",
      "2022-07-25 21:51:54 Epoch finished! Loss: 0.00718\n",
      "2022-07-25 21:51:54 Train loss: 0.007 (best)\n",
      "2022-07-25 21:51:54 Starting epoch 3/8.\n",
      "mean loss: 0.0049: 100%|████████████████████████████████████████████████| 640/640 [40:19<00:00,  3.78s/it]\n",
      "2022-07-25 22:32:15 Epoch finished! Loss: 0.00488\n",
      "2022-07-25 22:32:15 Train loss: 0.005 (best)\n",
      "2022-07-25 22:32:15 Starting epoch 4/8.\n",
      "mean loss: 0.0037: 100%|████████████████████████████████████████████████| 640/640 [40:45<00:00,  3.82s/it]\n",
      "2022-07-25 23:13:00 Epoch finished! Loss: 0.00374\n",
      "2022-07-25 23:13:01 Train loss: 0.004 (best)\n",
      "2022-07-25 23:13:01 Starting epoch 5/8.\n",
      "mean loss: 0.0036:  67%|████████████████████████████████▎               | 430/640 [27:14<13:10,  3.76s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    logger.info(f\"Starting epoch {epoch + 1}/{epochs}.\")\n",
    "\n",
    "    train_loss = train(model, optimizer, criterion, train_dataloader, logger, device)\n",
    "    if train_loss < best_model_info[\"train_loss\"]:\n",
    "        with open(os.path.join(output_dir, \"CP-best.pth\"), \"wb\") as fp:\n",
    "            torch.save(model.state_dict(), fp)\n",
    "        logger.info(f\"Train loss: {train_loss:.3f} (best)\")\n",
    "    else:\n",
    "        logger.info(f\"Train loss: {train_loss:.5f} (best {best_model_info['train_loss']:.5f})\")\n",
    "\n",
    "with open(os.path.join(output_dir, \"CP-last.pth\"), \"wb\") as fp:\n",
    "    torch.save(model.state_dict(), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720d0df-6ff6-4d6e-81c8-48f83a5dcb32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
